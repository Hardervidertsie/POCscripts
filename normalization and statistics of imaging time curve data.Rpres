


A practical methodology to normalization and statistics of time response imaging data
========================================================
author: ir. Steven Wink, PhD
date: 13-06-2016

The story (I)
========================================================

<br>
Over the last few years, I have worked on imaging data.. alot.
<br>
Every now and then some AUC and t-tests were executed.
<br>
  
Dreadfull significance of controls due to slightly different laser settings was the result.  
<br>  
Redoing the imaging was out of the question, normalization <strong>had</strong> to be improved
A test was needed that could capture the measurement error per time curve better. 
<br>
But this would cost time, so much precious time. And I was out of time...
Then  
<br>
Then, one day, something happened:

The story (II)
========================================================
<br>
A reviewer dared to ask for statistical analysis of our imaging time series data.  
<br>
Yes, A dreadfull day, and much moarning followed.   

But it was also a great challenge.  
<br> 
struggle and new insights occured in many a late night.
<br>
 
/_end of melodrama_

In this presentation I would like to share some insights and code.  



Solutions to problematic / non- perfect imaging data:
========================================================

- replicate with overal lower or higher responses
- saturation of imaging signals
- signals from different reporters
  - foci counts
  - cytoplamsa  
  - nuclei signal  
  - counting cells above a certain threshold  
- time dynamics and AUC
- varying time points  




Multiple plates & time dynamics
========================================================

![](presentation/figures/rawdata.png)
***
  <br>
Test set data: the problems are numerous  

-replicate 1 overal lower  

-replicate 3 is saturated for high response & has higher variance  

-unequal total imaging time  

-unequal time intervals  



The plate with unhappy cells
========================================================

![](presentation/figures/rawdata_unhappy.png)
***
  <br>
  <br>
  
overal lower responses  



The normal plate, beyond the reach of mere PhD students :)
========================================================

![](presentation/figures/rawdata_normal.png)
***
  <br>
  <br>
-No plateau  
-Low variance over time  
-Good use of dynamic range  
    
  <br> 
  <br>
Well-calibrated imaging session, highest response level does not reach a plateau.
Low-level responses are still visible.


The "I should have done this one over" plate
========================================================
![](presentation/figures/rawdata_highoutlier.png)
***
<br>
<br>

-High variance   
-low amount of time points   
-high-level response reaches plateau very early: does not capture
entire dynamic range. 


Linear normalization: equal plate means
========================================================
![](presentation/figures/meanShift.png)
***
<br>
<br>
$$ \Large x_{refP} = \\ \frac{1}{n_{refPlate}} \sum_{i=1, j = j_{refPlate}}^{i=i} x_{i,j} $$
<br>
$$ \Large x_{meanShift} = x_{j} + ( x_{refP} - x_{j mean} )  $$
<br>
Better, but clearly a horizontal shift will not be sufficient.


Min max plate normalization
========================================================
![](presentation/figures/minmaxNorm.png)
***
<br>
$$ \LARGE x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}  $$
<br>
Great for scaling plates for visual purposes.   
However this method is sensitive to the outlier points.  
Also the plateau is problematic.
<br>
<-- replicate 2 (green) at tp = 0 for our 'treathigh', ensures our control goes up.  

Thus low significance over-all.

Background subtraction
========================================================
![](presentation/figures/controlSub.png)
***
Generaly fixes the controls but not the responses. 
--Small responses end up more significant than higher more variable responses.

Linear normalizations are not sufficient in this case.
========================================================

The plate specific time-dynamics is different because of:
- plateau reach (suturation detector)  
- difference in laser settings do not lead to linear increase in background level  
- difference in laser settings do lead to linear increase in higher responses  


standard normal normalization ( Z-score )
========================================================
![](presentation/figures/standardNormalized.png)
***
$$ \Large Z = \frac{x - \bar{x}}{/sigma}
<br>
- Often used normalization method.  
- Data looks much better, statistics would make sense.  
- Confusing for interpretation, below zero, above 1.  
- A solution would be to use the sd-normal for statistics, but not for display.  


Non linear normalization: sigmoid
========================================================


***
xx <- seq(0,1, length.out = 100)

xdf <- data.frame( xx= xx, yy = sigmoidFun( xx, A = 0.1 , 0.5))
plot( xdf )




Quantile normalization
========================================================

Statistical considerations
========================================================
-AUC replicates, loose information of variance within time
-Thus very sensitive to non-perfect plate normalizations
-Long small response differences > time dynamics such as temporary peaks
-Early high peak less significant than very low prolonged response?


Available R-code
========================================================
-All the normalization formula's are available (this presentation)
-The R- code is neatly! (and this is very rare) formatted in Rmd file "norm and stats imaging data.Rmd"
-Includes code how to generate the data, or ...
- input your own data and find out what works best for you.

Future work
========================================================

- In 3 dimensions: time, dose and significance of responses: 3D plots with significance shading!


